{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_maze\n",
    "maze = gym.make('Maze4-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-05 17:41:50,690\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "WARNING:ray.tune.utils.util:Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0; avg. reward=227.27272727272728;\n",
      "EVALUATION MEAN REWARD: 280.0; EVALUATION MEAN STEPS: 40.76\n",
      "Iter: 1; avg. reward=346.9387755102041;\n",
      "EVALUATION MEAN REWARD: 840.0; EVALUATION MEAN STEPS: 26.16\n",
      "Iter: 2; avg. reward=569.8924731182796;\n",
      "EVALUATION MEAN REWARD: 960.0; EVALUATION MEAN STEPS: 16.6\n",
      "Iter: 3; avg. reward=890.0;\n",
      "EVALUATION MEAN REWARD: 920.0; EVALUATION MEAN STEPS: 19.8\n",
      "Iter: 4; avg. reward=1000.0;\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 8.28\n",
      "Iter: 5; avg. reward=1000.0;\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 7.0\n",
      "Iter: 6; avg. reward=1000.0;\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 7.2\n",
      "Iter: 7; avg. reward=1000.0;\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 5.36\n",
      "Iter: 8; avg. reward=995.0738916256157;\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.6\n",
      "Iter: 9; avg. reward=995.9677419354839;\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.68\n",
      "Iter: 10; avg. reward=996.2121212121212;\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.96\n",
      "Iter: 11; avg. reward=1000.0;\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.6\n",
      "Iter: 12; avg. reward=1000.0;\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.68\n",
      "Iter: 13; avg. reward=996.0474308300395;\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 4.44\n",
      "Iter: 14; avg. reward=1000.0;\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.2\n",
      "Iter: 15; avg. reward=1000.0;\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.88\n",
      "Iter: 16; avg. reward=1000.0;\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.4\n",
      "Iter: 17; avg. reward=1000.0;\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.36\n",
      "Iter: 18; avg. reward=1000.0;\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.84\n",
      "Iter: 19; avg. reward=1000.0;\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.36\n",
      "Iter: 20; avg. reward=1000.0;\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.84\n",
      "Iter: 21; avg. reward=1000.0;\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 4.08\n",
      "Iter: 22; avg. reward=996.2406015037594;\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.52\n",
      "Iter: 23; avg. reward=996.2962962962963;\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.4\n",
      "Iter: 24; avg. reward=1000.0;\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.68\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "def maze_env_creator(env_config):\n",
    "    maze.reset()\n",
    "    return maze  # return an env instance\n",
    "\n",
    "register_env(\"maze_env\", maze_env_creator)\n",
    "\n",
    "# Create an RLlib Trainer instance to learn how to act in the above\n",
    "# environment.\n",
    "trainer = DQNTrainer(\n",
    "    config={\n",
    "        # Env class to use (here: our gym.Env sub-class from above).\n",
    "        \"env\": \"maze_env\",\n",
    "        # Config dict to be passed to our custom env's constructor.\n",
    "        \"env_config\": {\n",
    "        },\n",
    "        # Parallelize environment rollouts.\n",
    "        \"num_workers\": 0,\n",
    "        \"n_step\": 10,\n",
    "        \"noisy\": True,\n",
    "        \"num_atoms\": 4,\n",
    "        \"v_min\": -10.0, \n",
    "        \"v_max\": 10.0\n",
    "        # \"evaluation_duration_unit\": \"episodes\",\n",
    "        # \"evaluation_duration\": 1,\n",
    "        # \"evaluation_num_episodes\": 1,\n",
    "        # \"min_train_timesteps_per_reporting\": 1,\n",
    "        # \"min_sample_timesteps_per_reporting\": 1,\n",
    "        # \"timesteps_per_iteration\": 1,\n",
    "        # \"min_time_s_per_reporting\": 1\n",
    "    })\n",
    "\n",
    "# print(trainer.config)\n",
    "# print(trainer.config[\"evaluation_duration\"])\n",
    "# print(trainer.config[\"evaluation_duration\"])\n",
    "\n",
    "for i in range(25):\n",
    "    results = trainer.train()\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']};\") # episodes_steps={results['hist_stats']['episode_lengths']}\")\n",
    "\n",
    "    total_rewards = []\n",
    "    total_steps = []\n",
    "    for i in range(25):\n",
    "        env = gym.make('Maze4-v0')\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        episode_steps = 0\n",
    "        # Play one episode.\n",
    "        while not done:\n",
    "            # Compute a single action, given the current observation\n",
    "            # from the environment.\n",
    "            action = trainer.compute_single_action(obs)\n",
    "            # Apply the computed action in the environment.\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            # Sum up rewards for reporting purposes.\n",
    "            total_reward += reward\n",
    "            episode_steps += 1\n",
    "        # Report results.\n",
    "        total_rewards.append(total_reward)\n",
    "        total_steps.append(episode_steps)\n",
    "        # print(f\"Shreaked for 1 episode; total-reward={total_reward}; steps={episode_steps}\")\n",
    "    \n",
    "    print(f\"EVALUATION MEAN REWARD: {sum(total_rewards) / len(total_rewards)}; EVALUATION MEAN STEPS: {sum(total_steps) / len(total_steps)}\")\n",
    "\n",
    "# env = gym.make('Maze4-v0')\n",
    "# obs = env.reset()\n",
    "# done = False\n",
    "# total_reward = 0.0\n",
    "# # Play one episode.\n",
    "# while not done:\n",
    "#     # Compute a single action, given the current observation\n",
    "#     # from the environment.\n",
    "#     action = trainer.compute_single_action(obs)\n",
    "#     # Apply the computed action in the environment.\n",
    "#     obs, reward, done, info = env.step(action)\n",
    "#     # Sum up rewards for reporting purposes.\n",
    "#     total_reward += reward\n",
    "# # Report results.\n",
    "# print(f\"Shreaked for 1 episode; total-reward={total_reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "64302c86e8c7bb2737f234d5097cf894aae7e8c1d85b127a10df9e9864e4954e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
