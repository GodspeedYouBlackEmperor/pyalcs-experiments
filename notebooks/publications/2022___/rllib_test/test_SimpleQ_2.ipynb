{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_maze\n",
    "maze = gym.make('Maze4-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=18292)\u001b[0m 2022-06-05 04:53:11,429\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "2022-06-05 04:53:14,438\tINFO trainable.py:152 -- Trainable.setup took 14.647 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-06-05 04:53:14,442\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0; avg. reward=416.6666666666667\n",
      "EVALUATION MEAN REWARD: 160.0; EVALUATION MEAN STEPS: 45.8\n",
      "Iter: 1; avg. reward=444.44444444444446\n",
      "EVALUATION MEAN REWARD: 640.0; EVALUATION MEAN STEPS: 28.72\n",
      "Iter: 2; avg. reward=615.3846153846154\n",
      "EVALUATION MEAN REWARD: 960.0; EVALUATION MEAN STEPS: 16.72\n",
      "Iter: 3; avg. reward=940.0\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 11.48\n",
      "Iter: 4; avg. reward=1000.0\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 7.2\n",
      "Iter: 5; avg. reward=1000.0\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 7.56\n",
      "Iter: 6; avg. reward=1000.0\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 6.4\n",
      "Iter: 7; avg. reward=1000.0\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 4.52\n",
      "Iter: 8; avg. reward=1000.0\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 4.32\n",
      "Iter: 9; avg. reward=1000.0\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.56\n",
      "Iter: 10; avg. reward=1000.0\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.52\n",
      "Iter: 11; avg. reward=991.5966386554621\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.96\n",
      "Iter: 12; avg. reward=995.7446808510638\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 4.0\n",
      "Iter: 13; avg. reward=1000.0\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.56\n",
      "Iter: 14; avg. reward=996.09375\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.8\n",
      "Iter: 15; avg. reward=991.869918699187\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.92\n",
      "Iter: 16; avg. reward=1000.0\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 4.0\n",
      "Iter: 17; avg. reward=1000.0\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.64\n",
      "Iter: 18; avg. reward=1000.0\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.48\n",
      "Iter: 19; avg. reward=1000.0\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.96\n",
      "Iter: 20; avg. reward=1000.0\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.96\n",
      "Iter: 21; avg. reward=1000.0\n",
      "EVALUATION MEAN REWARD: 960.0; EVALUATION MEAN STEPS: 5.0\n",
      "Iter: 22; avg. reward=975.1243781094528\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.8\n",
      "Iter: 23; avg. reward=1000.0\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.88\n",
      "Iter: 24; avg. reward=1000.0\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.4\n",
      "Iter: 25; avg. reward=1000.0\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.56\n",
      "Iter: 26; avg. reward=1000.0\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.76\n",
      "Iter: 27; avg. reward=1000.0\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.72\n",
      "Iter: 28; avg. reward=1000.0\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.88\n",
      "Iter: 29; avg. reward=1000.0\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 3.28\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.agents.dqn import SimpleQTrainer\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "def maze_env_creator(env_config):\n",
    "    maze.reset()\n",
    "    return maze  # return an env instance\n",
    "\n",
    "register_env(\"maze_env\", maze_env_creator)\n",
    "\n",
    "# Create an RLlib Trainer instance to learn how to act in the above\n",
    "# environment.\n",
    "trainer = SimpleQTrainer(\n",
    "    config={\n",
    "        # Env class to use (here: our gym.Env sub-class from above).\n",
    "        \"env\": \"maze_env\",\n",
    "        # Config dict to be passed to our custom env's constructor.\n",
    "        \"env_config\": {\n",
    "            # \"parrot_shriek_range\": gym.spaces.Box(-5.0, 5.0, (1, ))\n",
    "        },\n",
    "        # Parallelize environment rollouts.\n",
    "        \"num_workers\": 1,\n",
    "    })\n",
    "# Train for n iterations and report results (mean episode rewards).\n",
    "# Since we have to guess 10 times and the optimal reward is 0.0\n",
    "# (exact match between observation and action value),\n",
    "# we can expect to reach an optimal episode reward of 0.0.\n",
    "for i in range(30):\n",
    "    results = trainer.train()\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")\n",
    "\n",
    "\n",
    "\n",
    "    total_rewards = []\n",
    "    total_steps = []\n",
    "    for i in range(25):\n",
    "        env = gym.make('Maze4-v0')\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        episode_steps = 0\n",
    "        # Play one episode.\n",
    "        while not done:\n",
    "            # Compute a single action, given the current observation\n",
    "            # from the environment.\n",
    "            action = trainer.compute_single_action(obs)\n",
    "            # Apply the computed action in the environment.\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            # Sum up rewards for reporting purposes.\n",
    "            total_reward += reward\n",
    "            episode_steps += 1\n",
    "        # Report results.\n",
    "        total_rewards.append(total_reward)\n",
    "        total_steps.append(episode_steps)\n",
    "        # print(f\"Shreaked for 1 episode; total-reward={total_reward}; steps={episode_steps}\")\n",
    "    \n",
    "    print(f\"EVALUATION MEAN REWARD: {sum(total_rewards) / len(total_rewards)}; EVALUATION MEAN STEPS: {sum(total_steps) / len(total_steps)}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "64302c86e8c7bb2737f234d5097cf894aae7e8c1d85b127a10df9e9864e4954e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
