{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_maze\n",
    "maze = gym.make('Maze4-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=6996)\u001b[0m 2022-06-05 04:50:42,282\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "2022-06-05 04:50:46,842\tINFO trainable.py:152 -- Trainable.setup took 11.582 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-06-05 04:50:46,846\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0; avg. reward=384.61538461538464\n",
      "EVALUATION MEAN REWARD: 160.0; EVALUATION MEAN STEPS: 45.52\n",
      "Iter: 1; avg. reward=425.9259259259259\n",
      "EVALUATION MEAN REWARD: 680.0; EVALUATION MEAN STEPS: 24.04\n",
      "Iter: 2; avg. reward=476.1904761904762\n",
      "EVALUATION MEAN REWARD: 840.0; EVALUATION MEAN STEPS: 24.12\n",
      "Iter: 3; avg. reward=600.0\n",
      "EVALUATION MEAN REWARD: 160.0; EVALUATION MEAN STEPS: 45.4\n",
      "Iter: 4; avg. reward=560.0\n",
      "EVALUATION MEAN REWARD: 720.0; EVALUATION MEAN STEPS: 27.16\n",
      "Iter: 5; avg. reward=700.0\n",
      "EVALUATION MEAN REWARD: 1000.0; EVALUATION MEAN STEPS: 10.44\n",
      "Iter: 6; avg. reward=850.0\n",
      "EVALUATION MEAN REWARD: 360.0; EVALUATION MEAN STEPS: 43.04\n",
      "Iter: 7; avg. reward=780.0\n",
      "EVALUATION MEAN REWARD: 400.0; EVALUATION MEAN STEPS: 40.0\n",
      "Iter: 8; avg. reward=750.0\n",
      "EVALUATION MEAN REWARD: 880.0; EVALUATION MEAN STEPS: 17.52\n",
      "Iter: 9; avg. reward=770.0\n",
      "EVALUATION MEAN REWARD: 440.0; EVALUATION MEAN STEPS: 29.28\n",
      "Iter: 10; avg. reward=710.0\n",
      "EVALUATION MEAN REWARD: 600.0; EVALUATION MEAN STEPS: 25.04\n",
      "Iter: 11; avg. reward=630.0\n",
      "EVALUATION MEAN REWARD: 760.0; EVALUATION MEAN STEPS: 15.52\n",
      "Iter: 12; avg. reward=710.0\n",
      "EVALUATION MEAN REWARD: 800.0; EVALUATION MEAN STEPS: 14.32\n",
      "Iter: 13; avg. reward=730.0\n",
      "EVALUATION MEAN REWARD: 760.0; EVALUATION MEAN STEPS: 15.16\n",
      "Iter: 14; avg. reward=670.0\n",
      "EVALUATION MEAN REWARD: 520.0; EVALUATION MEAN STEPS: 27.2\n",
      "Iter: 15; avg. reward=660.0\n",
      "EVALUATION MEAN REWARD: 520.0; EVALUATION MEAN STEPS: 27.96\n",
      "Iter: 16; avg. reward=750.0\n",
      "EVALUATION MEAN REWARD: 760.0; EVALUATION MEAN STEPS: 15.88\n",
      "Iter: 17; avg. reward=830.0\n",
      "EVALUATION MEAN REWARD: 960.0; EVALUATION MEAN STEPS: 6.64\n",
      "Iter: 18; avg. reward=886.7924528301887\n",
      "EVALUATION MEAN REWARD: 760.0; EVALUATION MEAN STEPS: 17.84\n",
      "Iter: 19; avg. reward=896.2264150943396\n",
      "EVALUATION MEAN REWARD: 840.0; EVALUATION MEAN STEPS: 12.96\n",
      "Iter: 20; avg. reward=820.0\n",
      "EVALUATION MEAN REWARD: 760.0; EVALUATION MEAN STEPS: 15.28\n",
      "Iter: 21; avg. reward=770.0\n",
      "EVALUATION MEAN REWARD: 960.0; EVALUATION MEAN STEPS: 6.0\n",
      "Iter: 22; avg. reward=660.0\n",
      "EVALUATION MEAN REWARD: 720.0; EVALUATION MEAN STEPS: 18.24\n",
      "Iter: 23; avg. reward=690.0\n",
      "EVALUATION MEAN REWARD: 800.0; EVALUATION MEAN STEPS: 13.52\n",
      "Iter: 24; avg. reward=820.0\n",
      "EVALUATION MEAN REWARD: 840.0; EVALUATION MEAN STEPS: 12.44\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "def maze_env_creator(env_config):\n",
    "    maze.reset()\n",
    "    return maze  # return an env instance\n",
    "\n",
    "register_env(\"maze_env\", maze_env_creator)\n",
    "\n",
    "# Create an RLlib Trainer instance to learn how to act in the above\n",
    "# environment.\n",
    "trainer = DQNTrainer(\n",
    "    config={\n",
    "        # Env class to use (here: our gym.Env sub-class from above).\n",
    "        \"env\": \"maze_env\",\n",
    "        # Config dict to be passed to our custom env's constructor.\n",
    "        \"env_config\": {\n",
    "        },\n",
    "        # Parallelize environment rollouts.\n",
    "        \"num_workers\": 1,\n",
    "        \"prioritized_replay\": False,\n",
    "        \"dueling\": False,\n",
    "        \"double_q\": False,\n",
    "        \"n_step\": 20,\n",
    "    })\n",
    "\n",
    "# print(trainer.config)\n",
    "# print(trainer.config[\"evaluation_duration\"])\n",
    "# print(trainer.config[\"evaluation_duration\"])\n",
    "\n",
    "for i in range(25):\n",
    "    results = trainer.train()\n",
    "    # print(results)\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    total_rewards = []\n",
    "    total_steps = []\n",
    "    for i in range(25):\n",
    "        env = gym.make('Maze4-v0')\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        episode_steps = 0\n",
    "        # Play one episode.\n",
    "        while not done:\n",
    "            # Compute a single action, given the current observation\n",
    "            # from the environment.\n",
    "            action = trainer.compute_single_action(obs)\n",
    "            # Apply the computed action in the environment.\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            # Sum up rewards for reporting purposes.\n",
    "            total_reward += reward\n",
    "            episode_steps += 1\n",
    "        # Report results.\n",
    "        total_rewards.append(total_reward)\n",
    "        total_steps.append(episode_steps)\n",
    "        # print(f\"Shreaked for 1 episode; total-reward={total_reward}; steps={episode_steps}\")\n",
    "    \n",
    "    print(f\"EVALUATION MEAN REWARD: {sum(total_rewards) / len(total_rewards)}; EVALUATION MEAN STEPS: {sum(total_steps) / len(total_steps)}\")\n",
    "\n",
    "\n",
    "\n",
    "# env = gym.make('Maze4-v0')\n",
    "# obs = env.reset()\n",
    "# done = False\n",
    "# total_reward = 0.0\n",
    "# # Play one episode.\n",
    "# while not done:\n",
    "#     # Compute a single action, given the current observation\n",
    "#     # from the environment.\n",
    "#     action = trainer.compute_single_action(obs)\n",
    "#     # Apply the computed action in the environment.\n",
    "#     obs, reward, done, info = env.step(action)\n",
    "#     # Sum up rewards for reporting purposes.\n",
    "#     total_reward += reward\n",
    "# # Report results.\n",
    "# print(f\"Shreaked for 1 episode; total-reward={total_reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "64302c86e8c7bb2737f234d5097cf894aae7e8c1d85b127a10df9e9864e4954e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
