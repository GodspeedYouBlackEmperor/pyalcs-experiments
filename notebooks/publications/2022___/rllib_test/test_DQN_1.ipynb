{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_maze\n",
    "maze = gym.make('Maze4-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=23216)\u001b[0m 2022-06-04 20:04:00,374\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "2022-06-04 20:04:04,891\tINFO trainable.py:152 -- Trainable.setup took 12.354 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-06-04 20:04:04,896\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0; avg. reward=260.8695652173913\n",
      "Iter: 1; avg. reward=311.1111111111111\n",
      "Iter: 2; avg. reward=347.22222222222223\n",
      "Iter: 3; avg. reward=460.0\n",
      "Iter: 4; avg. reward=520.0\n",
      "Iter: 5; avg. reward=580.0\n",
      "Iter: 6; avg. reward=600.0\n",
      "Iter: 7; avg. reward=580.0\n",
      "Iter: 8; avg. reward=560.0\n",
      "Iter: 9; avg. reward=500.0\n",
      "Iter: 10; avg. reward=410.0\n",
      "Iter: 11; avg. reward=300.0\n",
      "Iter: 12; avg. reward=220.0\n",
      "Iter: 13; avg. reward=130.0\n",
      "Iter: 14; avg. reward=90.0\n",
      "Iter: 15; avg. reward=50.0\n",
      "Iter: 16; avg. reward=50.0\n",
      "Iter: 17; avg. reward=10.0\n",
      "Iter: 18; avg. reward=10.0\n",
      "Iter: 19; avg. reward=20.0\n",
      "Iter: 20; avg. reward=20.0\n",
      "Iter: 21; avg. reward=30.0\n",
      "Iter: 22; avg. reward=40.0\n",
      "Iter: 23; avg. reward=60.0\n",
      "Iter: 24; avg. reward=70.0\n",
      "Iter: 25; avg. reward=130.0\n",
      "Iter: 26; avg. reward=140.0\n",
      "Iter: 27; avg. reward=130.0\n",
      "Iter: 28; avg. reward=100.0\n",
      "Iter: 29; avg. reward=90.0\n",
      "Iter: 30; avg. reward=20.0\n",
      "Iter: 31; avg. reward=10.0\n",
      "Iter: 32; avg. reward=0.0\n",
      "Iter: 33; avg. reward=0.0\n",
      "Iter: 34; avg. reward=0.0\n",
      "Iter: 35; avg. reward=0.0\n",
      "Iter: 36; avg. reward=0.0\n",
      "Iter: 37; avg. reward=20.0\n",
      "Iter: 38; avg. reward=30.0\n",
      "Iter: 39; avg. reward=50.0\n",
      "Iter: 40; avg. reward=50.0\n",
      "Iter: 41; avg. reward=100.0\n",
      "Iter: 42; avg. reward=90.0\n",
      "Iter: 43; avg. reward=80.0\n",
      "Iter: 44; avg. reward=60.0\n",
      "Iter: 45; avg. reward=80.0\n",
      "Iter: 46; avg. reward=50.0\n",
      "Iter: 47; avg. reward=40.0\n",
      "Iter: 48; avg. reward=50.0\n",
      "Iter: 49; avg. reward=70.0\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "def maze_env_creator(env_config):\n",
    "    maze.reset()\n",
    "    return maze  # return an env instance\n",
    "\n",
    "register_env(\"maze_env\", maze_env_creator)\n",
    "\n",
    "# Create an RLlib Trainer instance to learn how to act in the above\n",
    "# environment.\n",
    "trainer = DQNTrainer(\n",
    "    config={\n",
    "        # Env class to use (here: our gym.Env sub-class from above).\n",
    "        \"env\": \"maze_env\",\n",
    "        # Config dict to be passed to our custom env's constructor.\n",
    "        \"env_config\": {\n",
    "        },\n",
    "        # Parallelize environment rollouts.\n",
    "        \"num_workers\": 1,\n",
    "        \"prioritized_replay\": False,\n",
    "        \"dueling\": False,\n",
    "        \"double_q\": False,\n",
    "    })\n",
    "\n",
    "for i in range(50):\n",
    "    results = trainer.train()\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")\n",
    "\n",
    "\n",
    "\n",
    "# env = gym.make('Maze4-v0')\n",
    "# obs = env.reset()\n",
    "# done = False\n",
    "# total_reward = 0.0\n",
    "# # Play one episode.\n",
    "# while not done:\n",
    "#     # Compute a single action, given the current observation\n",
    "#     # from the environment.\n",
    "#     action = trainer.compute_single_action(obs)\n",
    "#     # Apply the computed action in the environment.\n",
    "#     obs, reward, done, info = env.step(action)\n",
    "#     # Sum up rewards for reporting purposes.\n",
    "#     total_reward += reward\n",
    "# # Report results.\n",
    "# print(f\"Shreaked for 1 episode; total-reward={total_reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "64302c86e8c7bb2737f234d5097cf894aae7e8c1d85b127a10df9e9864e4954e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
